{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"74e8e985f0d64e9e84822ba9a36e6a58","deepnote_cell_type":"markdown"},"source":"#### Text data Preprocessing\n\n- **nltk** library(Natural Language Toolkit)를 이용하여 Text Processing을 위한 전처리를 실습한다.","block_group":"74e8e985f0d64e9e84822ba9a36e6a58"},{"cell_type":"markdown","metadata":{"cell_id":"74faa890d9014f42b4b26169d9dd5dfd","deepnote_cell_type":"markdown"},"source":"\n#### 1. 영어 문장 토큰화하기","block_group":"74faa890d9014f42b4b26169d9dd5dfd"},{"cell_type":"code","metadata":{"cell_id":"3e4074e873194304a60f99f46c94f9ea","source_hash":"c6a340e1","execution_start":1679613675789,"execution_millis":1300,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# NLTK는 Anaconda 설치 시 이미 설치되어 있으므로 별도 설치가 불필요합니다.\n# !pip install nltk==3.6.1\n\n# Test processing을 위해 nltk package 를 import\nimport nltk","block_group":"3e4074e873194304a60f99f46c94f9ea","execution_count":1,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"416946cd6c5f4a89805b749667a61272","deepnote_cell_type":"markdown"},"source":"아래 명령어를 통해 download 대화상자를 열어 패키지를 다운로드 받아야 합니다.\n<br>인터넷 속도 저하 시 매우 오래 걸리므로, 패키지 설치 경로만 확인한 다음 \\[ (nltk, downloaded) nltk_data.zip ] 의 파일들을 복사합니다.\n<br><br>경로 예시 : **\"C:\\Users\\{컴퓨터 이름}\\AppData\\Roaming\\nltk_data\"**\n<br>\n**<p style='color:red;'>nltk_data 폴더 안에 corpora, taggers, tokenizers 폴더가 바로 위치하도록 복사해줘야 합니다.</p>**\n","block_group":"416946cd6c5f4a89805b749667a61272"},{"cell_type":"code","metadata":{"cell_id":"983c5f40a94c4689b2cbf135b783353c","source_hash":"313a8acc","execution_start":1679613677089,"execution_millis":413,"is_output_hidden":true,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"nltk.download(\"popular\")  # 텍스트 데이터 처리를 위한 패키지 다운로더\n\n# Download following packages\n# Corpora : stopwords, wordnet\n# Models : averaged_perceptron_tagger, maxnet_treebank_pos_tagger, punkt","block_group":"983c5f40a94c4689b2cbf135b783353c","execution_count":2,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading collection 'popular'\n[nltk_data]    | \n[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to /root/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package names to /root/nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n[nltk_data]    |   Package shakespeare is already up-to-date!\n[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package treebank to /root/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package omw to /root/nltk_data...\n[nltk_data]    |   Package omw is already up-to-date!\n[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n[nltk_data]    |   Package omw-1.4 is already up-to-date!\n[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n[nltk_data]    |   Package wordnet2021 is already up-to-date!\n[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n[nltk_data]    |   Package wordnet31 is already up-to-date!\n[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to /root/nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package punkt to /root/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package snowball_data is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /root/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection popular\n","output_type":"stream"},{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"True"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/a174b5ea-5440-4b87-9464-1421033df20a","content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"88e86cc9055b4a8d92b29d0f6c8c8654","deepnote_cell_type":"markdown"},"source":"![image.png](attachment:image.png)","block_group":"88e86cc9055b4a8d92b29d0f6c8c8654"},{"cell_type":"code","metadata":{"cell_id":"c01b71e6164c4498bb56ebff160818aa","source_hash":"ef1ee32e","execution_start":1679613677539,"execution_millis":2,"is_output_hidden":true,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# 전처리하고자 하는 문장을 String 변수로 저장한다\nsentence = 'NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.'\n\n# 각 문장을 토큰화한 결과를 출력한다\nnltk.word_tokenize(sentence)  # 문장을 '단어 수준에서' 토큰화해 출력한다 ","block_group":"c01b71e6164c4498bb56ebff160818aa","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"['NLTK',\n 'is',\n 'a',\n 'leading',\n 'platform',\n 'for',\n 'building',\n 'Python',\n 'programs',\n 'to',\n 'work',\n 'with',\n 'human',\n 'language',\n 'data',\n '.',\n 'It',\n 'provides',\n 'easy-to-use',\n 'interfaces',\n 'to',\n 'over',\n '50',\n 'corpora',\n 'and',\n 'lexical',\n 'resources',\n 'such',\n 'as',\n 'WordNet',\n ',',\n 'along',\n 'with',\n 'a',\n 'suite',\n 'of',\n 'text',\n 'processing',\n 'libraries',\n 'for',\n 'classification',\n ',',\n 'tokenization',\n ',',\n 'stemming',\n ',',\n 'tagging',\n ',',\n 'parsing',\n ',',\n 'and',\n 'semantic',\n 'reasoning',\n ',',\n 'wrappers',\n 'for',\n 'industrial-strength',\n 'NLP',\n 'libraries',\n ',',\n 'and',\n 'an',\n 'active',\n 'discussion',\n 'forum',\n '.']"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/71bbd527-c7b9-42db-b4ed-8e46884571f9","content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"fedfbf0dc37b472786dc6828b51845d5","deepnote_cell_type":"markdown"},"source":"<br>\n<br>\n\n#### 2. 영어 문장 품사 태깅(POS tagging)하기","block_group":"fedfbf0dc37b472786dc6828b51845d5"},{"cell_type":"code","metadata":{"cell_id":"9728a8ff26c340deadad969964216d9e","source_hash":"a8f1633c","execution_start":1679613677604,"execution_millis":462,"is_output_hidden":true,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# 각 문장을 토큰화한 후 품사를 태깅하여 결과를 출력한다\n\ntokens = nltk.word_tokenize(sentence)  # 문장을 토큰화한다\nnltk.pos_tag(tokens)  # 토큰화한 문장을 대상으로 품사를 태깅(\"POS\" Tagging)하여 출력한다","block_group":"9728a8ff26c340deadad969964216d9e","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"[('NLTK', 'NNP'),\n ('is', 'VBZ'),\n ('a', 'DT'),\n ('leading', 'VBG'),\n ('platform', 'NN'),\n ('for', 'IN'),\n ('building', 'VBG'),\n ('Python', 'NNP'),\n ('programs', 'NNS'),\n ('to', 'TO'),\n ('work', 'VB'),\n ('with', 'IN'),\n ('human', 'JJ'),\n ('language', 'NN'),\n ('data', 'NNS'),\n ('.', '.'),\n ('It', 'PRP'),\n ('provides', 'VBZ'),\n ('easy-to-use', 'JJ'),\n ('interfaces', 'NNS'),\n ('to', 'TO'),\n ('over', 'IN'),\n ('50', 'CD'),\n ('corpora', 'NNS'),\n ('and', 'CC'),\n ('lexical', 'JJ'),\n ('resources', 'NNS'),\n ('such', 'JJ'),\n ('as', 'IN'),\n ('WordNet', 'NNP'),\n (',', ','),\n ('along', 'IN'),\n ('with', 'IN'),\n ('a', 'DT'),\n ('suite', 'NN'),\n ('of', 'IN'),\n ('text', 'NN'),\n ('processing', 'NN'),\n ('libraries', 'NNS'),\n ('for', 'IN'),\n ('classification', 'NN'),\n (',', ','),\n ('tokenization', 'NN'),\n (',', ','),\n ('stemming', 'VBG'),\n (',', ','),\n ('tagging', 'VBG'),\n (',', ','),\n ('parsing', 'NN'),\n (',', ','),\n ('and', 'CC'),\n ('semantic', 'JJ'),\n ('reasoning', 'NN'),\n (',', ','),\n ('wrappers', 'NNS'),\n ('for', 'IN'),\n ('industrial-strength', 'JJ'),\n ('NLP', 'NNP'),\n ('libraries', 'NNS'),\n (',', ','),\n ('and', 'CC'),\n ('an', 'DT'),\n ('active', 'JJ'),\n ('discussion', 'NN'),\n ('forum', 'NN'),\n ('.', '.')]"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/e010d1c5-0d27-424e-a7b4-cd0772a9f334","content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"dc731362917e4c04b075032bfc0a9e97","deepnote_cell_type":"markdown"},"source":"![image.png](attachment:image.png)","block_group":"dc731362917e4c04b075032bfc0a9e97"},{"cell_type":"markdown","metadata":{"cell_id":"4874adcfca4a487db75c002ebda13805","deepnote_cell_type":"markdown"},"source":"![image.png](attachment:image.png)","block_group":"4874adcfca4a487db75c002ebda13805"},{"cell_type":"markdown","metadata":{"cell_id":"a1847c51ec9140bbac80c631ccaf6d5d","deepnote_cell_type":"markdown"},"source":"## POS of English\n\n- Noun : 명사 n~\n- Verb : 동사 v~\n- Adjective : 형용사 j~(a~)","block_group":"a1847c51ec9140bbac80c631ccaf6d5d"},{"cell_type":"markdown","metadata":{"cell_id":"6451195d8745459baa5f21117fc2491e","deepnote_cell_type":"markdown"},"source":"<br>\n<br>\n\n#### 3. Stopwords 제거하기","block_group":"6451195d8745459baa5f21117fc2491e"},{"cell_type":"code","metadata":{"cell_id":"3690d688533843279e08b17d0157a2fc","source_hash":"832474ca","execution_start":1679613677876,"execution_millis":191,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# nltk 모듈에서 Stopwords를 직접 불러온다\nfrom nltk.corpus import stopwords","block_group":"3690d688533843279e08b17d0157a2fc","execution_count":5,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"cell_id":"4b40e8f3b1ec4113886aa168bf8deae1","source_hash":"f8cddb29","execution_start":1679613677877,"execution_millis":190,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# 영어의 stopwords를 불러와 변수에 저장한다 (stopwords에 속하는 \"단어\" 리스트)\nstopWords = stopwords.words('english') # 지원 언어 목록 : stopwords.fileids()\n\n# How many stop words\n#print(len(stopWords))\n#print()\n\n# stop words 출력\n#print(stopWords)","block_group":"4b40e8f3b1ec4113886aa168bf8deae1","execution_count":6,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"cell_id":"4abf2f0b29bc4123a4744b30f0b743d6","source_hash":"7f95ba1c","execution_start":1679613677878,"execution_millis":189,"is_output_hidden":true,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#print(tokens)","block_group":"4abf2f0b29bc4123a4744b30f0b743d6","execution_count":7,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"cell_id":"75d5d5f6012e44fb86d3ad9150f3ebd8","source_hash":"36474553","execution_start":1679613677879,"execution_millis":188,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# 문장에서 stopwords 제거\n\n#result = []  # stopwords가 제거된 결과를 담기 위한 리스트를 생성한다\n\n#for token in tokens:  # for문을 통해 각각의 token이 stopwords인지 아닌지를 판별해 결과에 저장한다\n#    if token.lower() not in stopWords:  # 만약 소문자로 변환한 token이 stopWords 내에 없으면:\n#        result.append(token)  # token을 리스트에 더해준다\n\n#print(result)  # 결과를 출력한다","block_group":"75d5d5f6012e44fb86d3ad9150f3ebd8","execution_count":8,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"cell_id":"44fdd0dc26644c75a3636823d8951a14","source_hash":"9d1388fa","execution_start":1679613677880,"execution_millis":187,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# stopwords에 쉼표(,)와 마침표(.) 추가하여 다시 적용하기\n\nstop_words = stopwords.words(\"english\") # stop_words == list\nstop_words.append(',')\nstop_words.append('.')\n\nresult = []  # stopwords가 제거된 결과를 담기 위한 리스트를 생성한다\n\nfor token in tokens:  # for문을 통해 각각의 token이 stopwords인지 아닌지를 판별해 결과에 저장한다\n    if token.lower() not in stop_words:  # 만약 소문자로 변환한 token이 stopWords 내에 없으면:\n        result.append(token)  # token을 리스트에 첨부한다\n\nprint(result)  # 결과를 출력한다","block_group":"44fdd0dc26644c75a3636823d8951a14","execution_count":9,"outputs":[{"name":"stdout","text":"['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', 'provides', 'easy-to-use', 'interfaces', '50', 'corpora', 'lexical', 'resources', 'WordNet', 'along', 'suite', 'text', 'processing', 'libraries', 'classification', 'tokenization', 'stemming', 'tagging', 'parsing', 'semantic', 'reasoning', 'wrappers', 'industrial-strength', 'NLP', 'libraries', 'active', 'discussion', 'forum']\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/15ca8029-8fc2-4be0-bf8c-749844e8f981","content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"696fcf0254e24a709a1a375d392651a4","deepnote_cell_type":"markdown"},"source":"<br>\n<br>\n\n#### 4. 영화 리뷰 데이터 전처리하기 - Lemmatizing\n- Lemmatization : 단어의 형태소적 & 사전적 분석을 통해 파생적 의미를 제거하고, 어근에 기반하여 **기본 사전형인 lemma**를 찾는 것","block_group":"696fcf0254e24a709a1a375d392651a4"},{"cell_type":"code","metadata":{"cell_id":"3f2f77e4b1924987b226acc793f5f306","source_hash":"70975b8","execution_start":1679613677882,"execution_millis":185,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# WordNetLemmatizer 객체 생성\nlemmatizer = nltk.wordnet.WordNetLemmatizer()","block_group":"3f2f77e4b1924987b226acc793f5f306","execution_count":10,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"cell_id":"7ceb5d14aeb841b1923a8345ab43c681","source_hash":"270ffac7","execution_start":1679613677883,"execution_millis":2268,"is_output_hidden":true,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# WordNetLemmatize는 더 정확한 분석을 위해 PoS 정보를 추가로 입력받을 수 있음 (n : 명사 v : 동사 a : 형용사 r : 부사)\n# default == n(명사) 이므로 'cats', 'geese' 들은 기본명사형인 'cat','geese'로 결과가 출력됨\n# 'ran'은 동사를 나타내는 PoS 정보인 'v'를 함께 입력해주어야 제대로 결과를 확인할 수 있음\n# 'better'도 마찬가지로, '형용사(a)'라는 정보를 함께 입력해주어야 원형인 'good'을 제대로 출력해줌\n\n#print(lemmatizer.lemmatize(\"cats\")) # lemmatize한 결과를 출력한다\n#print(lemmatizer.lemmatize(\"geese\"))","block_group":"7ceb5d14aeb841b1923a8345ab43c681","execution_count":11,"outputs":[{"name":"stdout","text":"cat\ngoose\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/6c91b259-c6c1-4083-9090-d2bfebbc5958","content_dependencies":null},{"cell_type":"code","metadata":{"cell_id":"ec2fe5af14fd4e78a9f2d51ee8fcaafc","source_hash":"464aea76","execution_start":1679613680155,"execution_millis":5,"is_output_hidden":true,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#print(lemmatizer.lemmatize(\"better\"))\n#print(lemmatizer.lemmatize(\"better\", pos=\"a\"))","block_group":"ec2fe5af14fd4e78a9f2d51ee8fcaafc","execution_count":12,"outputs":[{"name":"stdout","text":"better\ngood\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/127011fb-9bce-4e0f-a68c-62225181eaae","content_dependencies":null},{"cell_type":"code","metadata":{"cell_id":"1726e91c65374d5f9f00f2180cdaf9b4","source_hash":"821d5d98","execution_start":1679613680164,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"#print(lemmatizer.lemmatize(\"ran\"))\n#print(lemmatizer.lemmatize(\"ran\", 'v'))","block_group":"1726e91c65374d5f9f00f2180cdaf9b4","execution_count":13,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"cell_id":"8aac6d5b70294260a3fa56cae89037d1","source_hash":"2b1f21bc","execution_start":1679614351532,"execution_millis":470,"is_output_hidden":true,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"source":"# Stopwords\nstop_words = stopwords.words(\"english\")\nstop_words.append(',')\nstop_words.append('.')\n\nfile = open('moviereview.txt', 'r', encoding='utf-8') # 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다\nlines = file.readlines()  # readlines 함수로 텍스트 파일의 내용을 읽어 리스트로 저장한다\n\nsentence = lines[1] \ntokens = nltk.word_tokenize(sentence)  \ntagged_tokens =  nltk.pos_tag(tokens)\n\n# for문을 통해 stopwords 제거와 lemmatization을 수행한다\nlemmas = []  # lemmatize한 결과를 담기 위한 리스트를 생성한다\nfor token, posp in tagged_tokens:  \n    if token.lower() not in stop_words and posp != '.':  # 소문자로 변환한 token이 stopwords에 없으면:\n        if posp.startswith('N'):\n            lemmas.append(lemmatizer.lemmatize(token, pos='n'))\n        elif posp.startswith('V'):\n            lemmas.append(lemmatizer.lemmatize(token, pos='v'))\n        elif posp.startswith('J'):\n            lemmas.append(lemmatizer.lemmatize(token, pos='a'))\n        else:\n            lemmas.append(lemmatizer.lemmatize(token))  # lemmatize한 결과를 리스트에 첨부한다\n\nprint('Lemmas of : ' + sentence)  # lemmatize한 결과를 출력한다\nnltk.pos_tag(lemmas)","block_group":"8aac6d5b70294260a3fa56cae89037d1","execution_count":23,"outputs":[{"name":"stdout","text":"Lemmas of : Despite being a huge comic book nerd I was not familiar with the Guardians before the first movie came out. I did some googling and upon learning about this sci-fi superhero team consisting of a talking raccoon and tree man I was hardly impressed.I finally got round to watching the first film and was blown away. It had a remarkable charm that even the other Marvel titles didn't have. It was filled with great humour, memorable moments and fit into the Marvel Universe long running story perfectly.Because of this I pumped by expectations up high for the sequel to a degree where it was almost guaranteed to fail yet somehow, someway it didn't.Not only did it meet my expectations but it exceeded them, GotG2 is amazing.Full of the same five star humour, being a visual treat and once again with an excellent soundtrack the film gripped me from the outset and delivered that charm all over again.This time including several industry veterans including Kurt Russell (Who has been on great form since his return) and Sylvester Stallone they fit in well and don't detract from the franchise as I feared they might.With cameo appearances along the way from the likes of Farscape (1999) lead Ben Browder to industry legend Seth Green as Howard the Duck this is a fun rollercoaster ride than left me positively gagging for more.This is one of those films I feel like I could rant about (In a positive way) for a while and so I'm going to resist the urge and merely say that Guardians 2 is a contender for the best Marvel movie, contested only by the first Avengers film.Masterpiece.The Good:Cast are great againCharm returnsExcellent nostalgic soundtrackLooks amazingThe Bad:Only one movie with Baby Groot? Noooo!They seem to have dropped Drax's taking everything literal jokes awayMichael Rosenbaum was wastedThings I Learnt From This Movie:Batista should have skipped over wrestling and just been an actor\n\n","output_type":"stream"},{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"[('Despite', 'IN'),\n ('huge', 'JJ'),\n ('comic', 'JJ'),\n ('book', 'NN'),\n ('nerd', 'NN'),\n ('familiar', 'JJ'),\n ('Guardians', 'NNP'),\n ('first', 'JJ'),\n ('movie', 'NN'),\n ('come', 'VBN'),\n ('googling', 'VBG'),\n ('upon', 'IN'),\n ('learn', 'JJ'),\n ('sci-fi', 'JJ'),\n ('superhero', 'NN'),\n ('team', 'NN'),\n ('consisting', 'VBG'),\n ('talk', 'NN'),\n ('raccoon', 'NN'),\n ('tree', 'NN'),\n ('man', 'NN'),\n ('hardly', 'RB'),\n ('impressed.I', 'VBZ'),\n ('finally', 'RB'),\n ('get', 'VB'),\n ('round', 'NN'),\n ('watch', 'NN'),\n ('first', 'RB'),\n ('film', 'NN'),\n ('blow', 'VB'),\n ('away', 'RB'),\n ('remarkable', 'JJ'),\n ('charm', 'NN'),\n ('even', 'RB'),\n ('Marvel', 'NNP'),\n ('title', 'VBP'),\n (\"n't\", 'RB'),\n ('fill', 'VB'),\n ('great', 'JJ'),\n ('humour', 'NN'),\n ('memorable', 'JJ'),\n ('moment', 'NN'),\n ('fit', 'NN'),\n ('Marvel', 'NNP'),\n ('Universe', 'NNP'),\n ('long', 'JJ'),\n ('run', 'NN'),\n ('story', 'NN'),\n ('perfectly.Because', 'IN'),\n ('pump', 'JJ'),\n ('expectation', 'NN'),\n ('high', 'JJ'),\n ('sequel', 'NN'),\n ('degree', 'NN'),\n ('almost', 'RB'),\n ('guarantee', 'NN'),\n ('fail', 'JJ'),\n ('yet', 'RB'),\n ('somehow', 'VBP'),\n ('someway', 'RB'),\n (\"didn't.Not\", 'JJ'),\n ('meet', 'JJ'),\n ('expectation', 'NN'),\n ('exceed', 'VBP'),\n ('GotG2', 'NNP'),\n ('amazing.Full', 'JJ'),\n ('five', 'CD'),\n ('star', 'NN'),\n ('humour', 'NN'),\n ('visual', 'JJ'),\n ('treat', 'NN'),\n ('excellent', 'JJ'),\n ('soundtrack', 'NN'),\n ('film', 'NN'),\n ('grip', 'NN'),\n ('outset', 'NN'),\n ('deliver', 'NN'),\n ('charm', 'NN'),\n ('again.This', 'NN'),\n ('time', 'NN'),\n ('include', 'VBP'),\n ('several', 'JJ'),\n ('industry', 'NN'),\n ('veteran', 'VBD'),\n ('include', 'VBP'),\n ('Kurt', 'NNP'),\n ('Russell', 'NNP'),\n ('(', '('),\n ('great', 'JJ'),\n ('form', 'NN'),\n ('since', 'IN'),\n ('return', 'NN'),\n (')', ')'),\n ('Sylvester', 'NNP'),\n ('Stallone', 'NNP'),\n ('fit', 'NN'),\n ('well', 'VBP'),\n (\"n't\", 'RB'),\n ('detract', 'VB'),\n ('franchise', 'VB'),\n ('fear', 'NN'),\n ('might.With', 'NN'),\n ('cameo', 'NN'),\n ('appearance', 'NN'),\n ('along', 'IN'),\n ('way', 'NN'),\n ('like', 'IN'),\n ('Farscape', 'NNP'),\n ('(', '('),\n ('1999', 'CD'),\n (')', ')'),\n ('lead', 'NN'),\n ('Ben', 'NNP'),\n ('Browder', 'NNP'),\n ('industry', 'NN'),\n ('legend', 'NN'),\n ('Seth', 'NNP'),\n ('Green', 'NNP'),\n ('Howard', 'NNP'),\n ('Duck', 'NNP'),\n ('fun', 'NN'),\n ('rollercoaster', 'NN'),\n ('ride', 'NN'),\n ('leave', 'VBP'),\n ('positively', 'RB'),\n ('gag', 'JJ'),\n ('more.This', 'NN'),\n ('one', 'CD'),\n ('film', 'NN'),\n ('feel', 'NN'),\n ('like', 'IN'),\n ('could', 'MD'),\n ('rant', 'VB'),\n ('(', '('),\n ('positive', 'JJ'),\n ('way', 'NN'),\n (')', ')'),\n (\"'m\", 'VBP'),\n ('go', 'JJ'),\n ('resist', 'NN'),\n ('urge', 'NN'),\n ('merely', 'RB'),\n ('say', 'VBP'),\n ('Guardians', 'NNS'),\n ('2', 'CD'),\n ('contender', 'NN'),\n ('best', 'RBS'),\n ('Marvel', 'NNP'),\n ('movie', 'NN'),\n ('contest', 'NN'),\n ('first', 'JJ'),\n ('Avengers', 'NNPS'),\n ('film.Masterpiece.The', 'RB'),\n ('Good', 'JJ'),\n (':', ':'),\n ('Cast', 'NNP'),\n ('great', 'JJ'),\n ('againCharm', 'NN'),\n ('returnsExcellent', 'NN'),\n ('nostalgic', 'JJ'),\n ('soundtrackLooks', 'VBZ'),\n ('amazingThe', 'JJ'),\n ('Bad', 'NNP'),\n (':', ':'),\n ('one', 'CD'),\n ('movie', 'NN'),\n ('Baby', 'NNP'),\n ('Groot', 'NNP'),\n ('Noooo', 'NNP'),\n ('seem', 'VBP'),\n ('drop', 'NN'),\n ('Drax', 'NNP'),\n (\"'s\", 'POS'),\n ('take', 'NN'),\n ('everything', 'NN'),\n ('literal', 'JJ'),\n ('joke', 'NN'),\n ('awayMichael', 'NN'),\n ('Rosenbaum', 'NNP'),\n ('wastedThings', 'NNS'),\n ('Learnt', 'NNP'),\n ('Movie', 'NNP'),\n (':', ':'),\n ('Batista', 'NNP'),\n ('skip', 'VBD'),\n ('wrestling', 'VBG'),\n ('actor', 'NN')]"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/b8691a84-589f-44bd-a66b-d0a4332cef1d","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=53674883-1018-459e-80e4-3fe92d693a3d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2023-03-24T00:45:35.340Z"},"deepnote_notebook_id":"ab09ef1a82f948cf8a78d3565aa67981"}}